{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingClient.ipynb",
      "provenance": [],
      "mount_file_id": "171TTkN3jmsr4jt0roYll100-q9nV_pBY",
      "authorship_tag": "ABX9TyNlevg9GAs8ScZTQ65i5JZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanhPham2411/Reinforcement-Learning-Gold-Miner/blob/master/scripts/TrainingClient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyCJTulWx5Gw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fe2f038-6e23-4178-de12-1cf414ea524c"
      },
      "source": [
        "cd /content/drive/My Drive/Sync/Reinforcement-Learning-Gold-Miner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Sync/Reinforcement-Learning-Gold-Miner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp15BwGnyvNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upbp_KTIxw1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from MinerGymEnv import MinerGymEnv\n",
        "from Model.DQNModel import DQN # A class of creating a deep q-learning model\n",
        "from MinerEnv import MinerEnv # A class of creating a communication environment between the DQN model and the GameMiner environment (GAME_SOCKET_DUMMY.py)\n",
        "from Memory import Memory # A class of creating a batch in order to store experiences for the training process\n",
        "\n",
        "import pandas as pd\n",
        "import datetime \n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTW1fOoz4UX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbb9f09b-9ac5-4535-a09d-01971d6386d9"
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_session"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function keras.backend.tensorflow_backend.set_session>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEjcGCtDxB7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOST = \"localhost\"\n",
        "PORT = 1111\n",
        "\n",
        "\n",
        "# Create header for saving DQN learning file\n",
        "now = datetime.datetime.now() #Getting the latest datetime\n",
        "header = [\"Ep\", \"Step\", \"Reward\", \"Total_reward\", \"Actdion\", \"Epsilon\", \"Done\", \"Termination_Code\"] #Defining header for the save file\n",
        "filename = \"Data/data_\" + now.strftime(\"%Y%m%d-%H%M\") + \".csv\" \n",
        "with open(filename, 'w') as f:\n",
        "    pd.DataFrame(columns=header).to_csv(f, encoding='utf-8', index=False, header=True)\n",
        "\n",
        "# Parameters for training a DQN model\n",
        "N_EPISODE = 10000 #The number of episodes for training\n",
        "MAX_STEP = 1000   #The number of steps for each episode\n",
        "BATCH_SIZE = 32   #The number of experiences for each replay \n",
        "MEMORY_SIZE = 100000 #The size of the batch for storing experiences\n",
        "SAVE_NETWORK = 100  # After this number of episodes, the DQN model is saved for testing later. \n",
        "INITIAL_REPLAY_SIZE = 1000 #The number of experiences are stored in the memory batch before starting replaying\n",
        "INPUTNUM = 209 #The number of input values for the DQN model\n",
        "ACTIONNUM = 6  #The number of actions output from the DQN model\n",
        "MAP_MAX_X = 21 #Width of the Map\n",
        "MAP_MAX_Y = 9  #Height of the Map\n",
        "DEBUG = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0zrNlZAfbJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a DQN model and a memory batch for storing experiences\n",
        "DQNAgent = DQN(INPUTNUM, ACTIONNUM)\n",
        "memory = Memory(MEMORY_SIZE)\n",
        "\n",
        "# Initialize environment\n",
        "minerEnv = MinerGymEnv(HOST, PORT,debug=DEBUG) #Creating a communication environment between the DQN model and the game environment (GAME_SOCKET_DUMMY.py)\n",
        "minerEnv.start()  # Connect to the game\n",
        "\n",
        "train = False #The variable is used to indicate that the replay starts, and the epsilon starts decrease.\n",
        "#Training Process\n",
        "#the main part of the deep-q learning agorithm \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23xjpbEziUqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e0cfd8d1-0231-4ad2-dc91-7647fb24ea13"
      },
      "source": [
        "ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m'code sample'\u001b[0m/   init_docker_env.sh   requirements.txt   \u001b[01;34mTrainedModels\u001b[0m/\n",
            " \u001b[01;34mData\u001b[0m/           \u001b[01;34mMaps\u001b[0m/                \u001b[01;34mscripts\u001b[0m/\n",
            " \u001b[01;34mdocs\u001b[0m/           README.md            \u001b[01;34msrc\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n5N2uCYfnpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ae54e3bf-935b-4d29-e74f-ef644af11905"
      },
      "source": [
        "DQNAgent.model.load_weights('TrainedModels/DQNmodel_20200726-1939_ep2200.h5')\n",
        "DQNAgent.target_model.load_weights('TrainedModels/DQNmodel_20200726-1939_ep2200.h5')\n",
        "\n",
        "minerEnv.reset() \n",
        "s = minerEnv.get_state()\n",
        "                        \n",
        "total_reward = 0\n",
        "terminate = False \n",
        "maxStep = minerEnv.state.mapInfo.maxStep \n",
        "\n",
        "for step in range(0, maxStep):\n",
        "    # action = DQNAgent.act(s)\n",
        "    q_predict = DQNAgent.model.predict(s.reshape(1,len(s)))\n",
        "    action = np.argmax(q_predict)  \n",
        "    \n",
        "    minerEnv.step(str(action))\n",
        "    s_next = minerEnv.get_state()  \n",
        "    reward = minerEnv.get_reward() \n",
        "    terminate = minerEnv.check_terminate()  \n",
        "\n",
        "    total_reward = total_reward + reward #Plus the reward to the total rewad of the episode\n",
        "    s = s_next #Assign the next state for the next step.\n",
        "\n",
        "    print(f'steps:{step} action:{action} q_predict:{q_predict}')\n",
        "    if terminate == True:\n",
        "        #If the episode ends, then go to the next episode\n",
        "        break\n",
        "#Print the training information after the episode\n",
        "print('Number of steps is: %d. Accumulated Reward = %.2f. Epsilon = %.2f .Termination code: %d' % (\n",
        "    step + 1, total_reward, DQNAgent.epsilon, terminate))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "steps:0 action:3 q_predict:[[14.337856   3.2386677 19.819118  29.563423  22.053093   5.3174124]]\n",
            "steps:1 action:3 q_predict:[[14.244743   3.6381357 19.84628   28.574999  20.406996   3.4428375]]\n",
            "steps:2 action:3 q_predict:[[13.796219   3.2086146 19.084955  27.717543  20.150787   3.762162 ]]\n",
            "steps:3 action:3 q_predict:[[13.498821   3.0011873 18.680761  27.368818  19.989891   3.8350916]]\n",
            "steps:4 action:3 q_predict:[[11.376065   2.7833967 17.033302  24.538347  17.530657   2.4213662]]\n",
            "steps:5 action:3 q_predict:[[ 8.71283    2.1607308 15.425867  20.71021   14.70153   -1.2131127]]\n",
            "steps:6 action:3 q_predict:[[ 7.856098   1.3043858 15.008596  19.247156  13.95612   -1.1949588]]\n",
            "steps:7 action:3 q_predict:[[ 7.3578405  1.1467979 14.966654  18.927147  13.856074  -1.7681893]]\n",
            "steps:8 action:3 q_predict:[[ 6.9282     0.8308905 14.747862  18.355257  13.491598  -2.1669035]]\n",
            "steps:9 action:3 q_predict:[[ 5.3715963  -0.11215611 14.029888   16.161106   11.920361   -4.231462  ]]\n",
            "Number of steps is: 10. Accumulated Reward = -119.00. Epsilon = 1.00 .Termination code: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N21SmcLWx2Pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "30a01340-2aaa-446d-f80a-776b84c9f361"
      },
      "source": [
        "return\n",
        "\n",
        "for episode_i in range(0, N_EPISODE):\n",
        "    try:\n",
        "        # Choosing a map in the list\n",
        "        mapID = np.random.randint(1, 6) #Choosing a map ID from 5 maps in Maps folder randomly\n",
        "        posID_x = np.random.randint(MAP_MAX_X) #Choosing a initial position of the DQN agent on X-axes randomly\n",
        "        posID_y = np.random.randint(MAP_MAX_Y) #Choosing a initial position of the DQN agent on Y-axes randomly\n",
        "        #Creating a request for initializing a map, initial position, the initial energy, and the maximum number of steps of the DQN agent\n",
        "        request = (\"map\" + str(mapID) + \",\" + str(posID_x) + \",\" + str(posID_y) + \",50,100\") \n",
        "        #Send the request to the game environment (GAME_SOCKET_DUMMY.py)\n",
        "        minerEnv.send_map_info(request)\n",
        "\n",
        "        # Getting the initial state\n",
        "        minerEnv.reset() #Initialize the game environment\n",
        "        s = minerEnv.get_state()#Get the state after reseting. \n",
        "                                #This function (get_state()) is an example of creating a state for the DQN model \n",
        "        total_reward = 0 #The amount of rewards for the entire episode\n",
        "        terminate = False #The variable indicates that the episode ends\n",
        "        maxStep = minerEnv.state.mapInfo.maxStep #Get the maximum number of steps for each episode in training\n",
        "        #Start an episde for training\n",
        "        for step in range(0, maxStep):\n",
        "            action = DQNAgent.act(s)  # Getting an action from the DQN model from the state (s)\n",
        "            minerEnv.step(str(action))  # Performing the action in order to obtain the new state\n",
        "            s_next = minerEnv.get_state()  # Getting a new state\n",
        "            reward = minerEnv.get_reward()  # Getting a reward\n",
        "            terminate = minerEnv.check_terminate()  # Checking the end status of the episode\n",
        "\n",
        "            # Add this transition to the memory batch\n",
        "            memory.push(s, action, reward, terminate, s_next)\n",
        "\n",
        "            # Sample batch memory to train network\n",
        "            if (memory.length > INITIAL_REPLAY_SIZE):\n",
        "                #If there are INITIAL_REPLAY_SIZE experiences in the memory batch\n",
        "                #then start replaying\n",
        "                batch = memory.sample(BATCH_SIZE) #Get a BATCH_SIZE experiences for replaying\n",
        "                DQNAgent.replay(batch, BATCH_SIZE)#Do relaying\n",
        "                train = True #Indicate the training starts\n",
        "            total_reward = total_reward + reward #Plus the reward to the total rewad of the episode\n",
        "            s = s_next #Assign the next state for the next step.\n",
        "\n",
        "            # Saving data to file\n",
        "            save_data = np.hstack(\n",
        "                [episode_i + 1, step + 1, reward, total_reward, action, DQNAgent.epsilon, terminate]).reshape(1, 7)\n",
        "            with open(filename, 'a') as f:\n",
        "                pd.DataFrame(save_data).to_csv(f, encoding='utf-8', index=False, header=False)\n",
        "            \n",
        "            if terminate == True:\n",
        "                #If the episode ends, then go to the next episode\n",
        "                break\n",
        "\n",
        "        # Iteration to save the network architecture and weights\n",
        "        if (np.mod(episode_i + 1, SAVE_NETWORK) == 0 and train == True):\n",
        "            DQNAgent.target_train()  # Replace the learning weights for target model with soft replacement\n",
        "            #Save the DQN model\n",
        "            now = datetime.datetime.now() #Get the latest datetime\n",
        "            DQNAgent.save_model(\"TrainedModels/\",\n",
        "                                \"DQNmodel_\" + now.strftime(\"%Y%m%d-%H%M\") + \"_ep\" + str(episode_i + 1))\n",
        "\n",
        "\n",
        "\n",
        "        #Print the training information after the episode\n",
        "        print('Episode %d ends. Number of steps is: %d. Accumulated Reward = %.2f. Epsilon = %.2f .Termination code: %d' % (\n",
        "            episode_i + 1, step + 1, total_reward, DQNAgent.epsilon, terminate))\n",
        "        \n",
        "        #Decreasing the epsilon if the replay starts\n",
        "        if train == True:\n",
        "            DQNAgent.update_epsilon()\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "\n",
        "        traceback.print_exc()\n",
        "        # print(\"Finished.\")\n",
        "        break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-bf475d607ea7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ]
    }
  ]
}