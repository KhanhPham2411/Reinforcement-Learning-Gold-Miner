{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingClient.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/KhanhPham2411/Reinforcement-Learning-Gold-Miner/blob/master/scripts/TrainingClient.ipynb",
      "authorship_tag": "ABX9TyPPLuK/p5x4bFgBkW/TmWUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanhPham2411/Reinforcement-Learning-Gold-Miner/blob/master/scripts/TrainingClient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyCJTulWx5Gw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d7b105d-47d9-41c2-ea91-c73e2abfdeb9"
      },
      "source": [
        "cd /content/drive/My Drive/Sync/Reinforcement-Learning-Gold-Miner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Sync/Reinforcement-Learning-Gold-Miner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp15BwGnyvNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upbp_KTIxw1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from MinerGymEnv import MinerGymEnv\n",
        "from Model.DQNModel import DQN # A class of creating a deep q-learning model\n",
        "from MinerEnv import MinerEnv # A class of creating a communication environment between the DQN model and the GameMiner environment (GAME_SOCKET_DUMMY.py)\n",
        "from Memory import Memory # A class of creating a batch in order to store experiences for the training process\n",
        "\n",
        "import pandas as pd\n",
        "import datetime \n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTW1fOoz4UX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c31b74d6-c621-415d-fca9-464dcb0ab96f"
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_session"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function keras.backend.tensorflow_backend.set_session>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEjcGCtDxB7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOST = \"localhost\"\n",
        "PORT = 1111\n",
        "\n",
        "\n",
        "# Create header for saving DQN learning file\n",
        "now = datetime.datetime.now() #Getting the latest datetime\n",
        "header = [\"Ep\", \"Step\", \"Reward\", \"Total_reward\", \"Actdion\", \"Epsilon\", \"Done\", \"Termination_Code\"] #Defining header for the save file\n",
        "filename = \"Data/data_\" + now.strftime(\"%Y%m%d-%H%M\") + \".csv\" \n",
        "with open(filename, 'w') as f:\n",
        "    pd.DataFrame(columns=header).to_csv(f, encoding='utf-8', index=False, header=True)\n",
        "\n",
        "# Parameters for training a DQN model\n",
        "N_EPISODE = 10000 #The number of episodes for training\n",
        "MAX_STEP = 1000   #The number of steps for each episode\n",
        "BATCH_SIZE = 32   #The number of experiences for each replay \n",
        "MEMORY_SIZE = 100000 #The size of the batch for storing experiences\n",
        "SAVE_NETWORK = 100  # After this number of episodes, the DQN model is saved for testing later. \n",
        "INITIAL_REPLAY_SIZE = 1000 #The number of experiences are stored in the memory batch before starting replaying\n",
        "INPUTNUM = 209 #The number of input values for the DQN model\n",
        "ACTIONNUM = 6  #The number of actions output from the DQN model\n",
        "MAP_MAX_X = 21 #Width of the Map\n",
        "MAP_MAX_Y = 9  #Height of the Map\n",
        "DEBUG = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0zrNlZAfbJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "10cf45d3-8b89-419f-ba25-095041812a89"
      },
      "source": [
        "# Initialize a DQN model and a memory batch for storing experiences\n",
        "DQNAgent = DQN(INPUTNUM, ACTIONNUM)\n",
        "memory = Memory(MEMORY_SIZE)\n",
        "\n",
        "# Initialize environment\n",
        "minerEnv = MinerGymEnv(HOST, PORT,debug=DEBUG) #Creating a communication environment between the DQN model and the game environment (GAME_SOCKET_DUMMY.py)\n",
        "minerEnv.start()  # Connect to the game\n",
        "\n",
        "train = False #The variable is used to indicate that the replay starts, and the epsilon starts decrease.\n",
        "#Training Process\n",
        "#the main part of the deep-q learning agorithm \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 300)               63000     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 1806      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 155,106\n",
            "Trainable params: 155,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 300)               63000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6)                 1806      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 155,106\n",
            "Trainable params: 155,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Connected to server.\n",
            "Found: map4\n",
            "Found: map2\n",
            "Found: map3\n",
            "Found: map1\n",
            "Found: map5\n",
            "Connected to server.\n",
            "Found: map4\n",
            "Found: map2\n",
            "Found: map3\n",
            "Found: map1\n",
            "Found: map5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23xjpbEziUqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "db953fba-0e07-46ba-f4ac-0e6408622a93"
      },
      "source": [
        "ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m'code sample'\u001b[0m/   init_docker_env.sh   README.md          \u001b[01;34msrc\u001b[0m/\n",
            " \u001b[01;34mData\u001b[0m/           \u001b[01;34mManualPickModels\u001b[0m/    requirements.txt   \u001b[01;34mTrainedModels\u001b[0m/\n",
            " \u001b[01;34mdocs\u001b[0m/           \u001b[01;34mMaps\u001b[0m/                \u001b[01;34mscripts\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n5N2uCYfnpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "b5ba3f5f-cff6-4ca9-8c23-815f17a1bc79"
      },
      "source": [
        "from random import random, randrange\n",
        "\n",
        "# DQNAgent.model.load_weights('TrainedModels/DQNmodel_20200726-1939_ep2200.h5')\n",
        "# DQNAgent.target_model.load_weights('TrainedModels/DQNmodel_20200726-1939_ep2200.h5')\n",
        "\n",
        "minerEnv.reset() \n",
        "print(f'MapInfo:{minerEnv.state.mapInfo.max_x}:{minerEnv.state.mapInfo.max_y}')\n",
        "\n",
        "s = minerEnv.get_state()\n",
        "                        \n",
        "total_reward = 0\n",
        "terminate = False \n",
        "maxStep = minerEnv.state.mapInfo.maxStep \n",
        "\n",
        "for step in range(0, maxStep):\n",
        "    print(f'\\nStatus:{minerEnv.state.status} Pos:{minerEnv.state.x}:{minerEnv.state.y}')\n",
        "    # action = DQNAgent.act(s)\n",
        "    q_predict = DQNAgent.model.predict(s.reshape(1,len(s)))\n",
        "    # action = np.argmax(q_predict)  \n",
        "    action = randrange(6)\n",
        "\n",
        "    minerEnv.step(action)\n",
        "    s_next = minerEnv.get_state()  \n",
        "    reward = minerEnv.get_reward() \n",
        "    terminate = minerEnv.check_terminate()  \n",
        "\n",
        "    total_reward = total_reward + reward #Plus the reward to the total rewad of the episode\n",
        "    s = s_next #Assign the next state for the next step.\n",
        "\n",
        "    print(f'steps:{step} action:{action} q_predict:{q_predict}')\n",
        "    if terminate == True:\n",
        "        #If the episode ends, then go to the next episode\n",
        "        break\n",
        "#Print the training information after the episode\n",
        "print(f'\\nStatus:{minerEnv.state.status} Pos:{minerEnv.state.x}:{minerEnv.state.y}')\n",
        "print(f'Number of steps is: {step+1} Accumulated Reward = {total_reward}. Epsilon = {DQNAgent.epsilon}.Termination code: {terminate}')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MapInfo:20:8\n",
            "\n",
            "Status:0 Pos:2:2\n",
            "steps:0 action:0 q_predict:[[ 66.250114  184.78178    -1.8811884 -28.059275  -15.748304   -1.2545829]]\n",
            "\n",
            "Status:0 Pos:1:2\n",
            "steps:1 action:5 q_predict:[[ 68.393974  181.18054     4.4228077 -27.014399  -18.466696    3.5572724]]\n",
            "\n",
            "Status:0 Pos:1:2\n",
            "steps:2 action:0 q_predict:[[ 66.55236   176.72887     1.4338665 -18.626423  -19.484015   -4.289633 ]]\n",
            "\n",
            "Status:0 Pos:0:2\n",
            "steps:3 action:4 q_predict:[[ 60.409946   176.1792      -0.65217876 -12.5941925  -23.970743\n",
            "   -4.7905784 ]]\n",
            "\n",
            "Status:0 Pos:0:2\n",
            "steps:4 action:1 q_predict:[[ 53.564865  175.19351    -2.466219   -6.739778  -28.98911    -4.4087944]]\n",
            "\n",
            "Status:0 Pos:1:2\n",
            "steps:5 action:5 q_predict:[[ 49.180286  180.43993    -1.3262196 -12.342042  -28.376526    3.5586524]]\n",
            "\n",
            "Status:0 Pos:1:2\n",
            "steps:6 action:5 q_predict:[[ 35.55593   179.57657   -10.797798   -1.160141  -33.3056     -3.2114716]]\n",
            "\n",
            "Status:0 Pos:1:2\n",
            "steps:7 action:1 q_predict:[[ 30.658886 179.2786   -16.544518   6.356092 -35.296864 -10.003231]]\n",
            "\n",
            "Status:0 Pos:2:2\n",
            "steps:8 action:2 q_predict:[[ 22.311707 172.36627  -26.106426  20.290237 -39.35321  -22.684383]]\n",
            "\n",
            "Status:2 Pos:2:1\n",
            "Number of steps is: 9 Accumulated Reward = -101. Epsilon = 1.Termination code: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N21SmcLWx2Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for episode_i in range(0, N_EPISODE):\n",
        "    try:\n",
        "        # # Choosing a map in the list\n",
        "        # mapID = np.random.randint(1, 6) #Choosing a map ID from 5 maps in Maps folder randomly\n",
        "        # posID_x = np.random.randint(MAP_MAX_X) #Choosing a initial position of the DQN agent on X-axes randomly\n",
        "        # posID_y = np.random.randint(MAP_MAX_Y) #Choosing a initial position of the DQN agent on Y-axes randomly\n",
        "        # #Creating a request for initializing a map, initial position, the initial energy, and the maximum number of steps of the DQN agent\n",
        "        # request = (\"map\" + str(mapID) + \",\" + str(posID_x) + \",\" + str(posID_y) + \",50,100\") \n",
        "        # #Send the request to the game environment (GAME_SOCKET_DUMMY.py)\n",
        "        # minerEnv.send_map_info(request)\n",
        "\n",
        "        # Getting the initial state\n",
        "        minerEnv.reset() #Initialize the game environment\n",
        "        s = minerEnv.get_state()#Get the state after reseting. \n",
        "                                #This function (get_state()) is an example of creating a state for the DQN model \n",
        "        total_reward = 0 #The amount of rewards for the entire episode\n",
        "        terminate = False #The variable indicates that the episode ends\n",
        "        maxStep = minerEnv.state.mapInfo.maxStep #Get the maximum number of steps for each episode in training\n",
        "        #Start an episde for training\n",
        "        for step in range(0, maxStep):\n",
        "            action = DQNAgent.act(s)  # Getting an action from the DQN model from the state (s)\n",
        "            minerEnv.step(str(action))  # Performing the action in order to obtain the new state\n",
        "            s_next = minerEnv.get_state()  # Getting a new state\n",
        "            reward = minerEnv.get_reward()  # Getting a reward\n",
        "            terminate = minerEnv.check_terminate()  # Checking the end status of the episode\n",
        "\n",
        "            # Add this transition to the memory batch\n",
        "            memory.push(s, action, reward, terminate, s_next)\n",
        "\n",
        "            # Sample batch memory to train network\n",
        "            if (memory.length > INITIAL_REPLAY_SIZE):\n",
        "                #If there are INITIAL_REPLAY_SIZE experiences in the memory batch\n",
        "                #then start replaying\n",
        "                batch = memory.sample(BATCH_SIZE) #Get a BATCH_SIZE experiences for replaying\n",
        "                DQNAgent.replay(batch, BATCH_SIZE)#Do relaying\n",
        "                train = True #Indicate the training starts\n",
        "            total_reward = total_reward + reward #Plus the reward to the total rewad of the episode\n",
        "            s = s_next #Assign the next state for the next step.\n",
        "\n",
        "            # Saving data to file\n",
        "            save_data = np.hstack(\n",
        "                [episode_i + 1, step + 1, reward, total_reward, action, DQNAgent.epsilon, terminate]).reshape(1, 7)\n",
        "            with open(filename, 'a') as f:\n",
        "                pd.DataFrame(save_data).to_csv(f, encoding='utf-8', index=False, header=False)\n",
        "            \n",
        "            if terminate == True:\n",
        "                #If the episode ends, then go to the next episode\n",
        "                break\n",
        "\n",
        "        # Iteration to save the network architecture and weights\n",
        "        if (np.mod(episode_i + 1, SAVE_NETWORK) == 0 and train == True):\n",
        "            DQNAgent.target_train()  # Replace the learning weights for target model with soft replacement\n",
        "            #Save the DQN model\n",
        "            now = datetime.datetime.now() #Get the latest datetime\n",
        "            DQNAgent.save_model(\"TrainedModels/\",\n",
        "                                \"DQNmodel_\" + now.strftime(\"%Y%m%d-%H%M\") + \"_ep\" + str(episode_i + 1))\n",
        "\n",
        "\n",
        "\n",
        "        #Print the training information after the episode\n",
        "        print('Episode %d ends. Number of steps is: %d. Accumulated Reward = %.2f. Epsilon = %.2f. Action = %.2f .Termination code: %d' % (\n",
        "            episode_i + 1, step + 1, total_reward, DQNAgent.epsilon, action, terminate))\n",
        "        \n",
        "        #Decreasing the epsilon if the replay starts\n",
        "        if train == True:\n",
        "            DQNAgent.update_epsilon()\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "\n",
        "        traceback.print_exc()\n",
        "        # print(\"Finished.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}